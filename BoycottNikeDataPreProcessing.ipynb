{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BoycottNikeDataPreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tofCYqUqLnWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q xlrd\n",
        "!pip install -U -q PyDrive\n",
        "!pip install tweet-preprocessor\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import re #regular expression\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaCUtI2JO8Ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# will ask for login cred. and give you a pass code to insert here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mUmIxW_5jtO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Cleaning the data, no need to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7dz-HDoWS5o",
        "colab_type": "code",
        "outputId": "a3ff06a5-26a7-41f1-95f4-9454836f9370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# cleans the raw data\n",
        "\n",
        "file_id = '1LkqrL6v3g0HK6BOHoThQ9aRaNc_J85Wj' # the nike.xlsx file on gdrive\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('exported.xlsx')\n",
        "\n",
        "df = pd.read_excel('exported.xlsx')\n",
        "df\n",
        "\n",
        "# text cleaning \n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.MENTION,p.OPT.HASHTAG)\n",
        "\n",
        "cleaned=[]\n",
        "for i in range (len(df['tweet'])):\n",
        "  item=df['tweet'][i]\n",
        "  if '<Emoji' in item:\n",
        "    item =re.sub(\"<.*?>\",\"\",item)\n",
        "  item=p.clean(item)\n",
        "  item =re.sub(\"‚Äô\",\"\",item)\n",
        "  item =re.sub(\"‚Ä¶\",\"\",item)\n",
        "  item =re.sub(\"‚Äú\",\"\",item)\n",
        "  item =re.sub(\"‚Å¶\",\"\",item)\n",
        "  item =re.sub(\"‚Å©\",\"\",item)\n",
        "  item =re.sub(\"‚Äî\",\"\",item)\n",
        "  item =re.sub(\"‚Äù\",\"\",item)\n",
        "  \n",
        "  df['tweet'][i]=item\n",
        "\n",
        "# save the clean data in an xlsx file and download\n",
        "#df=pd.DataFrame(cleaned,index=[i for i in range(0,len(cleaned))], columns=['text'])\n",
        "#df.to_excel('cleanData.xlsx')\n",
        "#!dir\n",
        "#files.download('cleanData.xlsx')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  exported.xlsx  output.xlsx  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgOVzgF857dp",
        "colab_type": "text"
      },
      "source": [
        "# Categorizing tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaiWvfD5brAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the clean data\n",
        "file_id = '1NNQf225_ovzYoDYI9nid0HvZHfOqvOiZ' # the cleanText.xlsx file on gdrive\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('cleanData.xlsx')\n",
        "\n",
        "df = pd.read_excel('cleanData.xlsx')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIP-2rZ6sh1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mylist=[]\n",
        "cat1=\"liberty|free|First Amendment|1st amendment|constitutional rights|free speech|constitution|bill of rights|right|choice|choose|protest|freedom of speech|unconstitutional|FirstAmendment\"\n",
        "cat2=\"unity|united|together|solidarity|same|unite|our country|divide|division|divided|each other|one nation under God|support|us|community|ImWithKap|divisive\"\n",
        "cat3=\"equal|justice|racism|color|race|racial|slave|racist|bigot|equality|BlackLivesMatter|poverty|Black|Black athlete|humanity|white|supremacy|white supremacist|police brutality|hypocrisy|privilege|NoJusticeNoPeace|human goodness|slave narrative|slavery|racebaiting|MLK|oppression|social justice|injustice\"\n",
        "cat4=\"honor|respect|country|veteran|america|military|vet|soldier|patriot|hero|patriotism|armed forces|serve|served|integrity|army|uniform|grave|BlueLivesMatter|disrespect\"\n",
        "\n",
        "for i,item in df.iterrows():\n",
        "  if re.search(cat1, str(item['tweet']),re.IGNORECASE):\n",
        "    mylist.append(1)\n",
        "  elif re.search(cat2, str(item['tweet']),re.IGNORECASE):\n",
        "    mylist.append(2)\n",
        "  elif re.search(cat3, str(item['tweet']),re.IGNORECASE):\n",
        "    mylist.append(3)\n",
        "  elif re.search(cat4, str(item['tweet']),re.IGNORECASE):\n",
        "    mylist.append(4)\n",
        "  else:\n",
        "    mylist.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqLC_PbKsUYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add category index to the df \n",
        "newdf=df\n",
        "newdf.insert(0,column='cat',value=mylist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZis90mDsOcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the categorized data in an xlsx file and download\n",
        "writer = pd.ExcelWriter('categorized.xlsx')\n",
        "newdf.to_excel(writer, 'newsheet' )\n",
        "writer.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdX44VsuExGP",
        "colab_type": "code",
        "outputId": "5c09df8d-65de-4235-da6d-557b15ee2dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!dir\n",
        "files.download('categorized.xlsx')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  categorized.xlsx  cleanData.xlsx  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}